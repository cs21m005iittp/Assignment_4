/*


Q1.Consider matrix vector product where matrix is of large size expensive to be stored in single process so uniformly distribute matrix on multiple process
and perform marix vector product:
challenges
1.balanced distribution of matrix among process
2.gathering result 
3.same copy of vector present on each process for matrx vector product 

. Input: Size of the vector which also size for matrix. Output: resultant vector value (Print only once).
*/
#include <mpi.h>
#include <stdio.h>
#include <time.h>
#include <omp.h>
#include<iostream>
using namespace std;
double random_number_generator()
{
   double lbound = 0;
   double ubound = 10;
  
     return rand()%11;

}

int main(int argc, char** argv) {

	int n=0;
	n=atoi(argv[1]);
	
	
    // Initialize the MPI environment
    
    
    MPI_Init(&argc, &argv);
	//printf("%d,%c",argc,** argv);


    // Get the number of processes
    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    // Get the rank of the process
    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    // Get the name of the processor
    char processor_name[MPI_MAX_PROCESSOR_NAME];
    int name_len;
    MPI_Get_processor_name(processor_name, &name_len);
    
    clock_t start, end;
     double cpu_time_used;
    
    start = clock();
    
   
    int elements=n/world_size;
    //double vector[n];
	
	
	if(n>0)
	{
		//dividing by rows 
		int rows=n/world_size;
		int Extra_remaining_rows=n%world_size;
		if(Extra_remaining_rows>0&&rows>0)
		{
		
			if((world_size-world_rank)<=Extra_remaining_rows)//checking here remaining rows distributed in current process or not
			{
				rows+=1;
			}
		}
		
		//calculating rows taken by neibour allocatedrowsByNeibour
		int allocatedElementByNeibour=0;
	for(int i=0;i<=world_rank-1;i++)
	{
		if((world_size-i)<=Extra_remaining_rows)//checking here remaining element distributed in current process or not(for balanced distribution)
		{
			allocatedElementByNeibour+=1+rows;
		}
		else{
			allocatedElementByNeibour+=rows;
		}
	}
		
		if(rows>0)//checking whether i got some data to process
		{
			//declare matrix 
			//double A[rows][n];
			
			//allocating memory
			double** A = (double**)malloc(rows * sizeof(double*));
   			for (int i = 0; i < rows; i++)
    			A[i] = (double*)malloc(n * sizeof(double));
			//allocated memory
			
			//allocating memeory
			double* Result;
   			Result = (double*)malloc(rows * sizeof(double));
   			double* vector;
   			vector = (double*)malloc(n * sizeof(double));
   			double* finalresult;
   			finalresult = (double*)malloc(n * sizeof(double));
			//
			//double Result[n];
			int start_index=allocatedElementByNeibour;
			//int end_index=allocatedElementByNeibour+rows;//index for final result generation by specfic process
			//generate random data for matrix dim i got
			for(int i=0;i<rows;i++)
				{
					for(int j=0;j<n;j++)
					{
						A[i][j]=random_number_generator();
						//cout<<" A[i][j]="<<A[i][j]<<" process="<<world_rank;
					}
				}
				
			//if im process 0 then i will create vector which used by all other process for computation
			if(world_rank==0)
			{
				//generate vector
				//send to other process
				for(int i=0;i<n;i++)
				{
					vector[i]=random_number_generator();
					//cout<<" vector[i]="<<vector[i];
				}
				//broadcast vector to other process
				
				
				
				
			}
			MPI_Bcast(vector,n,MPI_DOUBLE,0,MPI_COMM_WORLD);
			{
				MPI_Barrier(MPI_COMM_WORLD);//no process start computation until vector generated by process zero  send 
				
			
			}
			/*
			cout<<"\ni am process="<<world_rank<<"started freeing memeory"<<endl;
			for(int i=0;i<n;i++)
				{
					cout<<" vector["<<i<<"]"<<vector[i];
				}
				*/
			//start computation:
			#pragma omp parallel for
			for(int i=0;i<rows;i++)
			{
				for(int j=0;j<n;j++)
				{
					Result[i]+=A[i][j]*vector[j];
				}
				
			}
			MPI_Barrier(MPI_COMM_WORLD);
			//cout<<" start index:"<<start_index<<" process="<<world_rank;
			//MPI_Gather(void* send_data,int send_count,MPI_Datatype send_datatype,void* recv_data,int recv_count,MPI_Datatype recv_datatype,int root,MPI_Comm communicator)
			if(world_size!=1)
			MPI_Gather(Result,rows,MPI_DOUBLE,finalresult,rows,MPI_DOUBLE,0,MPI_COMM_WORLD);//when its uneven distribution code stucks
			
			
			if(world_rank==0)
			{
				cout<<"result:"<<endl;
				for(int j=0;j<n;j++)
				{
					cout<<" "<<finalresult[j];
				}
				end = clock();
    				 cpu_time_used = ((double) (end - start)) / CLOCKS_PER_SEC;
     
    				 printf("\nTime:%f secs\n",cpu_time_used);
			}
			//barrier
			MPI_Barrier(MPI_COMM_WORLD);
			//free memory
			//cout<<"i am process="<<world_rank<<"started freeing memeory"<<endl;
			for (int i = 0; i < rows; i++)
			{
				free(A[i]);
			}
       		 
                      // cout<<"started free free(A);"<<endl;
    			free(A);
    			// cout<<"started free free(Result);;"<<endl;
    			free(Result);
    			// cout<<"started free free(vector);"<<endl;
    			free(vector);
    			free(finalresult);
			// cout<<"done free "<<endl;
		}
	}
	else
	{
		if(world_rank==0)
		{
			cout<<"invalid input received"<<endl;
		}
	}
	
           
            
	  //cout<<"executed success fully"<<endl;	   
    // Finalize the MPI environment.
    MPI_Finalize();
}
